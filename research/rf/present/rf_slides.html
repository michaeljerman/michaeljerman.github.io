<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title> Michael Jerman </title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/slide_white.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

	<!-- C-x C-k S for slide, M for markdown -->

	<section data-state='no-title-footer' data-transition="fade-out">
	  <h1>
	    Nonparametric Regression Discontinuity Estimation with Random Forest Kernels
	    </h1>
	  
	  <h2> Michael Jerman </h2>

	  <h6>  Department of Economics <br>
	    University of Oregon <br>
	    October 19, 2017 
	  </h6>
	</section>

	<section data-markdown>
	  ### Introduction

	  - Regression discontinuity (RD) is an increasingly common tool in applied econometrics 

	  - Many competing implementation methods (parametric and nonparametric)

	  - Results often sensitive to bandwidth and kernel choices

	  - "Best" specifications generally determined heuristically

	  - I develop a data-driven, adaptive kernels based on the "random forest" algorithm

	  - Kernel and bandwidth determined simultaneously, with minimal input from the researcher
	</section>

	<section data-markdown>
	  ### RD Methods

	  - When outcome variable is nonlinear in running variable, RD is usually estimated with "local linear" regression

	  - Run OLS on a subset of the data "near" the cutoff, and weigh observations closer to the cutoff more heavily

	  - Kernels are almost always triangular or rectangular (triangle is asymptotically optimal)

	  - Bandwidth chosen by cross-validation, Imbens-Kalyanaraman (IK), or simply looking at the data
	</section>

	<section>
	  <h4> Discontinuity Example </h4>
	  <img src="img/raw.png" height=550>
	</section>

	<section>
	  <h4> Discontinuity Example </h4>
	  <img src="img/raw_fit.png" height=550>
	</section>
	
	<section data-markdown>
	  ### Random Forest (RF) and Random Linear Forest (RLF) Kernels

	  - Recursively partition data into intervals by mean (RF) or linear relationship (RLF)

	  - Create partitions for *B* bootstraps

	  - The kernel around each *x* is the sum of every bootstrapped interval that *x* belongs to

	  - Estimate a linear model, using kernels as weights

	  - RF kernel assigns more weight to observations near each other in $y$ space (similar to cross-validation)
	  
	  - RLF kernel assigns more weight to observations along the same linear segment

	  
	</section>

	<section>
	  <h4> RLF Algorithm </h4>

	  <img src="img/lf_ex1.png" height=550>
	</section>

	<section>
	  <h4> RLF Algorithm </h4>

	  <img src="img/lf_ex2.png" height=550>
	</section>

	<section>
	  <h4> RLF Algorithm </h4>

	  <img src="img/lf_ex3.png" height=550>
	</section>
	

	<section>
	  <h4> Triangle Kernel, IK Bandwidth </h4>

	  <img src="img/ikrd.png" height=550> 
	</section>

	<section>
	  <h4> Random Linear Forest Kernel and Bandwidth </h4>

	  <img src="img/lfrd.png" height=550> 
	</section>

	<section>
	  <h4> Random Forest Kernel and Bandwidth </h4>

	  <img src="img/rflinear.png" height=550> 
	</section>
	
	<section data-markdown>
	  ### Simulations

	  - Sample size of $N = 100$, $x$ drawn from uniform distribution

	  - $\epsilon \sim N(0, 0.2^2)$
	  
	  - 500 Monte Carlo repetitions

	  - Relative efficiency: $\dfrac{MSE\_{1}}{MSE\_{2}}$

	  - Three models
	</section>

	<section>
	  <h4> Models </h4>

	    <img src='img/models.png' height=350>

	  
	</section>

	<section>
	  <h4> Simulation Results </h4>
	    <img src='img/tab1.png' height=200>
	  
	</section>

	<section data-markdown>
	  ### Future Work

	  - Multidimensional kernels 

	  - Compare quadratic RLF to local-quadratic methods

	  - Compare to "bias corrected" RD

	  - Bayesian regression trees: include prior information in a robust way

	  - Random polynomial trees

	</section>
	
	
      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
      // More info https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
      history: true,
      backgroundTransition: 'none',
      transition: 'none',
      viewDistance: 3,
      controls: true,
      progress: true,
      center: true,

      // More info https://github.com/hakimel/reveal.js#dependencies
      dependencies: [
      { src: 'plugin/title-footer/title-footer.js', async: true, callback: function() { title_footer.initialize('Michael Jerman |    Regression Discontinuity with Random Forest Kernels | October 20, 2017'); } },
      { src: 'plugin/chalkboard/chalkboard.js' },
      { src: 'plugin/markdown/marked.js' },
      { src: 'plugin/markdown/markdown.js' },
      { src: 'plugin/notes/notes.js', async: true },
      { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
      { src: 'plugin/math-katex/math-katex.js', async: true }
      ],

      keyboard: {
        67: function() { RevealChalkboard.toggleNotesCanvas() },
        66: function() { RevealChalkboard.toggleChalkboard() }, 
        46: function() { RevealChalkboard.clear() },    
         8: function() { RevealChalkboard.reset() },    
        68: function() { RevealChalkboard.download() }, 
      },

      chalkboard: { 
      theme: "whiteboard",
      color: [ 'rgba(0,0,0,1)', 'rgba(255,255,255,0.5)' ],
      pen: [ 'crosshair', 'crosshair' ]
      },
      
      });
    </script>
  </body>
</html>
